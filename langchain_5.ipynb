{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLC5OANTx0kQnEj7Hjafpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanushpachabhatla/My_LangChain_Playground/blob/main/langchain_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Agents Intro"
      ],
      "metadata": {
        "id": "9ytrcg1TMcuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNme4sUFKR3b",
        "outputId": "924e5e12-4b80-4bfc-b466-072f989feb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain langsmith langchain_google_genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-husky-bran-20\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSapd1kxNqHd",
        "outputId": "fe9c695e-fd69-4656-fc5d-ebfeb73bae14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass(\"Enter your GEMINI KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-2.5-flash',temperature= 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey8gx5ZsNtIY",
        "outputId": "c4f2f892-e431-4563-9ff1-a3d9967bf2dc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GEMINI KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Basics"
      ],
      "metadata": {
        "id": "5JcyVq4Uj_kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools are a way augment our LLMs with code execution. A tool is simply a function formatted so that our agent can undertstand how to use it, and then execute it.\n",
        "\n",
        "We can use the `@tool` decorator to create an LLM-compatible tool from a standard python function — this function should include a few things for optimal performance:"
      ],
      "metadata": {
        "id": "aJYlRu_ljg9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x"
      ],
      "metadata": {
        "id": "FV0XguU9hH8-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nctYJlEnjsWj",
        "outputId": "6e72de62-8982-4ac5-b00c-36f3316d0d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x7edf7d54f100>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{add.name=}\\n{add.description=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bh28p9njsxr",
        "outputId": "ab0cf0df-39ab-457b-901d-dcefb2040f4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add.name='add'\n",
            "add.description=\"Add 'x' and 'y'.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "add.args_schema.model_json_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxw6Kamj3o7",
        "outputId": "01320416-9b25-4e15-dabf-ce4c18ef7ac3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Add 'x' and 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'add',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exponentiate.args_schema.model_json_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwsGOkqoj6Y1",
        "outputId": "59fb9c2a-042b-49e6-e068-143a1bc94f86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': \"Raise 'x' to the power of 'y'.\",\n",
              " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
              "  'y': {'title': 'Y', 'type': 'number'}},\n",
              " 'required': ['x', 'y'],\n",
              " 'title': 'exponentiate',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "llm_output_string = \"{\\\"x\\\": 6, \\\"y\\\": 3}\"  # this is the output from the LLM\n",
        "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
        "llm_output_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-CVulZuj8-5",
        "outputId": "d6f76507-7b64-458c-ee10-980db12b885c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 6, 'y': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is then passed into the tool function as kwargs (keyword arguments) as indicated by the ** operator - the ** operator is used to unpack the dictionary into keyword arguments."
      ],
      "metadata": {
        "id": "cp9aDI7LkQz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exponentiate.func(**llm_output_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqeZmr8gkHLv",
        "outputId": "43f0321b-7750-4bed-a465-645d499f7482"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Creation"
      ],
      "metadata": {
        "id": "-K2EZlwVkTkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How It Works :\n",
        "\n",
        "1. The agent reads your query.\n",
        "\n",
        "2. It decides whether to use the Calculator tool or respond directly.\n",
        "\n",
        "3. It uses `ReAct` (Reasoning + Acting) style prompting to determine steps."
      ],
      "metadata": {
        "id": "AMgiX_oauy1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- for now create Tool calling agent\n",
        "\n",
        "- here we will have a scracthpad for llm to write down its thoughts while giving answer"
      ],
      "metadata": {
        "id": "vrNVRgxFk39i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"history\",  # must align with MessagesPlaceholder variable_name\n",
        "    return_messages=True  # to return Message objects\n",
        ")\n",
        "\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")"
      ],
      "metadata": {
        "id": "MNjjSzG8kVCn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"history\": memory.chat_memory.messages,\n",
        "    \"intermediate_steps\": []  # agent will append it's internal steps here\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKrEB5ikt5d",
        "outputId": "3ab91ca8-5d9c-4bef-ea6e-92eabf5e602e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ToolAgentAction(tool='multiply', tool_input={'y': 7.68, 'x': 10.7}, log=\"\\nInvoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"y\": 7.68, \"x\": 10.7}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--828a47bb-3eb5-4fae-8623-e5a7d390ae8e-0', tool_calls=[{'name': 'multiply', 'args': {'y': 7.68, 'x': 10.7}, 'id': '8141512d-ab05-4af2-8158-d27d8cc3da59', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 5, 'total_tokens': 96, 'input_token_details': {'cache_read': 0}})], tool_call_id='8141512d-ab05-4af2-8158-d27d8cc3da59')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM told us to use the multiply tool with inputs `{\"x\": 10.7, \"y\": 7.68}`. However, it didn't actually run the tool. For that, we need an agent execution loop, which manages the whole process of thinking, calling tools, and then thinking again. We'll use the `AgentExecutor` class to handle this for us."
      ],
      "metadata": {
        "id": "veXDeyXYlmqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "agent_executer = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "nbjIu0s1lw3J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try the same query with the executor, note that the intermediate_steps parameter that we added before is no longer needed as the executor handles it internally."
      ],
      "metadata": {
        "id": "Q8XIdFHPmPnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\n",
        "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
        "    \"history\": memory.chat_memory.messages,\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRNUFM0VmPXT",
        "outputId": "14d4af8d-ef7b-441d-b10e-0965042dbfd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `multiply` with `{'y': 7.68, 'x': 10.7}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m82.17599999999999\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 82.176.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is 10.7 multiplied by 7.68?',\n",
              " 'history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 82.176.\\n', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'The answer is 82.176.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `multiply` tool was invoked, producing the observation of `82.175999...`. After the observation was provided, we can see that the LLM then generated a final response of:\n",
        "\n",
        "```\n",
        "10.7 multiplied by 7.68 is approximately 82.18.\n",
        "```\n",
        "\n",
        "This final response was generated based on the original query and the tool output (ie the _observation_). We can also confirm that this answer is accurate:"
      ],
      "metadata": {
        "id": "xwsbcfMYmvyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\n",
        "    \"input\": \"My name is James\",\n",
        "    \"history\": memory\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz50TaufmwWl",
        "outputId": "c266497e-d2d5-4fc5-e39e-d7be867e58b8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mNice to meet you, James!  Is there anything I can help you with today?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is James',\n",
              " 'history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 82.176.\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, James!  Is there anything I can help you with today?\\n', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'Nice to meet you, James!  Is there anything I can help you with today?\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\n",
        "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
        "    \"history\": memory\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpJevRbdm9Fh",
        "outputId": "234125d1-92fa-4539-c752-49370b95f3bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mHere's how to solve that step-by-step:\n",
            "\n",
            "1. **Multiplication:** 4 * 2 = 8\n",
            "2. **Addition:** 9 + 10 = 19\n",
            "3. **Subtraction:** 19 - 8 = 11\n",
            "4. **Exponentiation:** 11³ = 1331\n",
            "\n",
            "Therefore, the answer is $\\boxed{1331}$\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
              " 'history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 82.176.\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, James!  Is there anything I can help you with today?\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Here's how to solve that step-by-step:\\n\\n1. **Multiplication:** 4 * 2 = 8\\n2. **Addition:** 9 + 10 = 19\\n3. **Subtraction:** 19 - 8 = 11\\n4. **Exponentiation:** 11³ = 1331\\n\\nTherefore, the answer is $\\\\boxed{1331}$\\n\", additional_kwargs={}, response_metadata={})],\n",
              " 'output': \"Here's how to solve that step-by-step:\\n\\n1. **Multiplication:** 4 * 2 = 8\\n2. **Addition:** 9 + 10 = 19\\n3. **Subtraction:** 19 - 8 = 11\\n4. **Exponentiation:** 11³ = 1331\\n\\nTherefore, the answer is $\\\\boxed{1331}$\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executer.invoke({\n",
        "    \"input\": \"What is my name\",\n",
        "    \"history\": memory\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtPbHwtDnAqr",
        "outputId": "5e12530d-fdfe-4305-f81a-8f4366a4d1bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mYour name is James.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name',\n",
              " 'history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The answer is 82.176.\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Nice to meet you, James!  Is there anything I can help you with today?\\n', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Here's how to solve that step-by-step:\\n\\n1. **Multiplication:** 4 * 2 = 8\\n2. **Addition:** 9 + 10 = 19\\n3. **Subtraction:** 19 - 8 = 11\\n4. **Exponentiation:** 11³ = 1331\\n\\nTherefore, the answer is $\\\\boxed{1331}$\\n\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James.\\n', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'Your name is James.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SerpAPI Weather Agent"
      ],
      "metadata": {
        "id": "87VVzobbnapA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WNA4ZhDxp9t2",
        "outputId": "8b7de3f6-80ef-4301-a634-387f54ffe5b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73oXcNmDr0vM",
        "outputId": "37cf1e9f-d8fd-464e-af53-9e9f333104f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = getpass(\"Enter your SerpAPI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAtyGQ6RnaB3",
        "outputId": "03c2d980-5bbe-4a78-f6e4-5379375b3c2d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your SerpAPI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "toolbox = load_tools(tool_names=['serpapi'], llm=llm)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rPMnMRBVoMO1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "method-1 : creating tool using `@tool`"
      ],
      "metadata": {
        "id": "AiO-H4yfvcng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "@tool\n",
        "def get_location_from_ip():\n",
        "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\"https://ipinfo.io/json\")\n",
        "        data = response.json()\n",
        "        print(\"raw data from ip - \",data)\n",
        "        if 'loc' in data:\n",
        "            latitude, longitude = data['loc'].split(',')\n",
        "            data = (\n",
        "                f\"Latitude: {latitude},\\n\"\n",
        "                f\"Longitude: {longitude},\\n\"\n",
        "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
        "                f\"Country: {data.get('country', 'N/A')}\"\n",
        "            )\n",
        "            return data\n",
        "        else:\n",
        "            return \"Location could not be determined.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {e}\"\n",
        "\n",
        "@tool\n",
        "def get_current_datetime() -> str:\n",
        "    \"\"\"Return the current date and time.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ],
      "metadata": {
        "id": "5IavP8FdoTnS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"you're a helpful assistant\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\")\n",
        "])"
      ],
      "metadata": {
        "id": "5lTNVT3gp0Jz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
        "\n",
        "agent = create_tool_calling_agent(\n",
        "    llm=llm, tools=tools, prompt=prompt\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "0UvWB9LQp1K0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = agent_executor.invoke({\n",
        "    \"input\": (\n",
        "        \"I have a few questions, what is the date and time right now? \"\n",
        "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
        "    )\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qezh53dMp4z-",
        "outputId": "911638e7-9c6e-4939-bf26-7a482dd8f398"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_current_datetime` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m2025-09-02 14:04:20\u001b[0m\u001b[32;1m\u001b[1;3mI am sorry, I cannot fulfill this request. I do not have the functionality to get your location and current weather.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic calculator Agent"
      ],
      "metadata": {
        "id": "tummYrgzrlle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "import math\n",
        "\n",
        "# Simple calculator tool\n",
        "def calculator_tool(query: str) -> str:\n",
        "    try:\n",
        "        return str(eval(query))\n",
        "    except:\n",
        "        return \"Error evaluating expression.\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=calculator_tool,\n",
        "        description=\"Evaluates mathematical expressions. Example: '2+2' or 'sqrt(16)'.\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "1YTEjF0Rq0oa"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* initialize the agent"
      ],
      "metadata": {
        "id": "B0xfKh1Cq-ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "sloS3RuxrBHg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Key Agent Types :\n",
        "\n",
        "- ZERO_SHOT_REACT_DESCRIPTION – LLM decides from descriptions.\n",
        "\n",
        "- CONVERSATIONAL_REACT_DESCRIPTION – Same but keeps chat history.\n",
        "\n",
        "- STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION – For structured outputs."
      ],
      "metadata": {
        "id": "8J_cngWRs7Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"What is 45 * 12, and then add 100?\")\n",
        "print(\"Agent Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKvYnF2vrD7p",
        "outputId": "01a40634-8092-4aeb-90f0-b0a125aa4d07"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2425042886.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = agent.run(\"What is 45 * 12, and then add 100?\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to multiply 45 by 12 and then add 100 to the result.  I can use the calculator for this.\n",
            "Action: Calculator\n",
            "Action Input: 45*12+100\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m640\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 640\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent Response: 640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realistic Example: Web Search + Calculator"
      ],
      "metadata": {
        "id": "cBPpaBEErqdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = SerpAPIWrapper()\n",
        "tools.append(\n",
        "    Tool(\n",
        "        name=\"Web Search\",\n",
        "        func=search.run,\n",
        "        description=\"Search the web for real-time information.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "hDazEEy7rrRM"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "LyU41r-jwap6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Find the population of France and multiply it by 2.\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNTL0j07r8MA",
        "outputId": "70ab4f63-50af-4846-9e47-cbdf3fd88d4c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: Web Search\n",
            "Action Input: population of France\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m{'type': 'population_result', 'population': '68.52 million', 'year': '2024'}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction: Calculator\n",
            "Action Input: 68.52 * 2\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m137.04\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The population of France multiplied by 2 is 137.04 million.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The population of France multiplied by 2 is 137.04 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversation Multi Tool Agent"
      ],
      "metadata": {
        "id": "Q_mHgjffwC2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "#use ConversationBufferMemory to store memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    memory=memory, #pass it here\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "response1 = agent.run(\"What is the population of India?  Give very recent & accurate data\")\n",
        "print(\"Response 1:\", response1)\n",
        "\n",
        "response2 = agent.run(\"Multiply that by 2 using Calculator.\")\n",
        "print(\"Response 2:\", response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ZzOnl9uJ7a",
        "outputId": "6ed7cace-8c50-4bb0-8b31-889089c72371"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: Web Search\n",
            "Action Input: current population of India\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m{'type': 'population_result', 'population': '1.451 billion', 'year': '2024'}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: The current population of India is estimated to be 1.451 billion in 2024.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response 1: The current population of India is estimated to be 1.451 billion in 2024.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
            "Action: Calculator\n",
            "Action Input: 1.451 * 2\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m2.902\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
            "AI: The population of India, multiplied by 2, is 2.902 billion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Response 2: The population of India, multiplied by 2, is 2.902 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How It Works :\n",
        "\n",
        "1. First query fetches real-time data using `Web Search`.\n",
        "\n",
        "2. Conversation `memory` stores that value.\n",
        "\n",
        "3. Second query recalls previous answer and uses `Calculator`.\n",
        "\n",
        "4. `verbose=True` shows agent reasoning."
      ],
      "metadata": {
        "id": "Kotx_qHk0zoz"
      }
    }
  ]
}