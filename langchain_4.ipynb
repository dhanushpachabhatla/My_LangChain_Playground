{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvS9eJXC56ks4jYDBYfkzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanushpachabhatla/My_LangChain_Playground/blob/main/langchain_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Memory for Langchain (GEMINI)"
      ],
      "metadata": {
        "id": "h0y9yJpVFwxB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyQhmjd9Qdkn",
        "outputId": "e08ace64-1329-4a41-e152-0ae6d7cae208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain langsmith langchain_google_genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-husky-bran-20\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0h5r5ozQveD",
        "outputId": "e6f124e6-884c-4845-a209-2dac78c9798f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass(\"Enter your GEMINI KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature= 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccWlqlVjQxbm",
        "outputId": "336936d2-0ceb-48d3-c678-3dc0c1eca3f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GEMINI KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conversational Memory in LangChain"
      ],
      "metadata": {
        "id": "8RvNMxg0UhnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory allows the LLM to remember previous interactions, essential for building chatbots or assistants with context."
      ],
      "metadata": {
        "id": "ARMetMdPUk1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How It Works\n",
        "\n",
        "* ConversationBufferMemory stores entire conversation history as plain text.\n",
        "\n",
        "* The model sees the previous conversation in every prompt."
      ],
      "metadata": {
        "id": "3VVJEGIPWHvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variants of Memory:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "\n",
        "* `ConversationBufferWindowMemory` – Remembers only the last N interactions (helps control token usage).\n",
        "\n",
        "* `ConversationTokenBufferMemory` – Remembers conversation up to a token limit.\n",
        "\n",
        "* `ConversationSummaryMemory` – Summarizes old history to save space.\n",
        "\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types."
      ],
      "metadata": {
        "id": "eiOQOr4YWMsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ConversationBufferMemory"
      ],
      "metadata": {
        "id": "h7BPJ-5AW4_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ],
      "metadata": {
        "id": "2WKIQKvGXDkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmrhKU6KXU4g",
        "outputId": "84ff7b9b-e1e0-457e-b91f-0f3fef0b5670"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ],
      "metadata": {
        "id": "NrQBzTgARJtd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ],
      "metadata": {
        "id": "3w-mONzUXJq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OawugfroXG16",
        "outputId": "fae7992f-053c-42ac-b3a7-d3e7c74f940b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* alternate approach"
      ],
      "metadata": {
        "id": "pUmKzIaJXOSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgAoV-jZXLdX",
        "outputId": "b0c44b9d-8e1a-46a6-b7aa-9954d41f15cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "both will give same output"
      ],
      "metadata": {
        "id": "GlTDxswZ_M2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtgoaSASXZuV",
        "outputId": "534c5ff1-4cb6-4a59-f6ab-18fda13b876e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3010654318.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
              " 'response': 'Your name is James.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationBufferMemeory with `RunnableWithMessageHistory`"
      ],
      "metadata": {
        "id": "WoNecAVsXfPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "steps:\n",
        "1. insert `MessagePlavceholder` in `ChatPromptTemplate` (while creating Prompt)\n",
        "\n",
        "2. Pipleline with prompt and llm (ECEL)\n",
        "\n",
        "3. Another pipeline which is object of `RunnableWithMessageHistory` which includes `pipeline`, `get_chat_history`,`input_message_key`, `history_message_key`\n",
        "\n",
        "4. invoke"
      ],
      "metadata": {
        "id": "36eNUjQ7D1zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "wO3boiQvXqQ0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = prompt_template | llm"
      ],
      "metadata": {
        "id": "PEqLWu1EAfoD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ],
      "metadata": {
        "id": "hGBLIKcGA5Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ],
      "metadata": {
        "id": "sU83HF25B8n7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "OYzMRCwSA2tk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOGfEVbeCvau",
        "outputId": "2c27dce4-4c5c-438e-9aac-f6d9f55900c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--29493119-1f08-4f79-98d1-60ac0bbae09e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 19, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ],
      "metadata": {
        "id": "_TVRkU4oCxeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjsLmdv_CyCD",
        "outputId": "7ec7c60f-aafc-4250-f2a7-f63ea46ee940"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--63533430-b597-4e33-a603-dbdc87d5d362-0', usage_metadata={'input_tokens': 38, 'output_tokens': 6, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#With runnableLamda"
      ],
      "metadata": {
        "id": "PPdC5lX-DOAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "steps:\n",
        "1. insert `MessagePlavceholder` in `ChatPromptTemplate` (while creating Prompt)\n",
        "\n",
        "2. create a pipleline with `input_var`(input, history) | `prompt` | `llm` | `output` | `saving_output` (function to load memory using `memory.save_context()` this function should be wrapped up by RunnableLambda to be use in ECEL pipeline)\n",
        "\n",
        "\n",
        "3. invoke"
      ],
      "metadata": {
        "id": "qMVm1ejRE2bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "ODU602AdDVWM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),  # Memory placeholder\n",
        "    (\"human\", \"{input}\")  # Latest user input\n",
        "])\n"
      ],
      "metadata": {
        "id": "2zjPe8tCC1uQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def save_context(input_output):\n",
        "    memory.save_context({\"input\": input_output[\"input\"]},\n",
        "                        {\"output\": input_output[\"output\"]})\n",
        "    return input_output[\"output\"]\n",
        "\n",
        "memory_runnable = RunnableLambda(save_context)\n",
        "\n",
        "chain = (\n",
        "    (lambda x: {\"input\": x[\"input\"], \"history\": memory.load_memory_variables({})[\"history\"]})\n",
        "    | prompt2\n",
        "    | llm\n",
        "    | (lambda x: {\"input\": x.content, \"output\": x.content})  # Wrap to pass to save_context\n",
        "    | memory_runnable\n",
        ")"
      ],
      "metadata": {
        "id": "f-npet62DJyQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"input\": \"Hi, who won the FIFA World Cup in 2018?\"}))\n",
        "print(chain.invoke({\"input\": \"What country hosted that tournament?\"}))\n",
        "print(chain.invoke({\"input\": \"Can you summarize what we talked about?\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xktxDU9LDfrQ",
        "outputId": "c9e51dbb-b598-49d1-b420-39ad7f95e8e3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "France won the FIFA World Cup in 2018.\n",
            "Russia hosted the 2018 FIFA World Cup.\n",
            "We discussed the 2018 FIFA World Cup, specifically that France won the tournament and Russia hosted it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.ConverationBufferMemory"
      ],
      "metadata": {
        "id": "W9ysPXb6FpZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fne_3le8Dut7",
        "outputId": "6bfdd4f4-bbb6-4554-9ec8-4f722376967d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "4pCcIj-8IiEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain_core.runnables import RunnableLambda\n"
      ],
      "metadata": {
        "id": "hMob8ELoIpMm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "\n",
        "# Summarizing memory uses an LLM to condense history\n",
        "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJVJ3Pu2IgvI",
        "outputId": "39d91159-ca64-42b3-f5ef-9d3a92038fa7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2718444277.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "W4R-afZrIq3s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_context(input_output):\n",
        "    memory.save_context(\n",
        "        {\"input\": input_output[\"input\"]},\n",
        "        {\"output\": input_output[\"output\"]}\n",
        "    )\n",
        "    return input_output[\"output\"]\n",
        "\n",
        "memory_runnable = RunnableLambda(save_context)\n",
        "\n",
        "chain = (\n",
        "    (lambda x: {\"input\": x[\"input\"], \"history\": memory.load_memory_variables({})[\"history\"]})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | (lambda x: {\"input\": x.content, \"output\": x.content})\n",
        "    | memory_runnable\n",
        ")\n"
      ],
      "metadata": {
        "id": "BCfzqFREIs-C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"input\": \"Explain quantum computing in simple terms.\"}))\n",
        "print(chain.invoke({\"input\": \"Who pioneered this field?\"}))\n",
        "print(chain.invoke({\"input\": \"Can you summarize what we've discussed so far?\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH0QZ_m3IumP",
        "outputId": "c6245061-1bff-42ce-f75d-966c54ee9407"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine a light switch. It can be either ON or OFF, right?  That's how regular computers work – using bits that are either 0 or 1.\n",
            "\n",
            "Quantum computers are different. They use **qubits**.  A qubit is like a special light switch that can be ON, OFF, or *both at the same time*! This \"both at the same time\" is called **superposition**.  It's like the light switch is flickering so fast you can't tell if it's on or off.\n",
            "\n",
            "Another cool thing qubits can do is **entanglement**. Imagine two of these special light switches linked magically. If one is ON, the other is instantly OFF, no matter how far apart they are.  This allows quantum computers to explore many possibilities simultaneously.\n",
            "\n",
            "Because of superposition and entanglement, quantum computers can solve certain types of problems much faster than regular computers.  Think of it like searching a maze: a regular computer tries each path one by one, while a quantum computer can explore all paths at once.\n",
            "\n",
            "However, quantum computers are still very new and experimental. They're not meant to replace regular computers for everything. They're best suited for specific, complex problems like:\n",
            "\n",
            "* **Drug discovery:** Simulating molecules to design new medicines.\n",
            "* **Materials science:** Designing new materials with specific properties.\n",
            "* **Financial modeling:** Creating more accurate and efficient financial models.\n",
            "* **Cryptography:** Breaking current encryption methods (and creating new, more secure ones).\n",
            "\n",
            "It's a bit like comparing a super-specialized tool to a general-purpose hammer.  A hammer is great for many things, but a specialized tool is much better for its specific task.  Quantum computers are that specialized tool.\n",
            "Pinpointing a single \"pioneer\" of quantum computing is difficult, as its development is the result of contributions from many physicists and computer scientists over several decades.  However, some key figures whose work laid crucial groundwork include:\n",
            "\n",
            "* **Richard Feynman (1918-1988):**  Often considered a foundational figure.  In a seminal 1982 lecture, he proposed that a quantum computer could simulate quantum systems far more efficiently than a classical computer, highlighting the potential power of the approach.  This is widely seen as a pivotal moment in the field's conceptual development.\n",
            "\n",
            "* **David Deutsch (born 1953):**  In 1985, he proposed the first theoretical model of a universal quantum computer, demonstrating that it could perform computations that classical computers couldn't. This provided a concrete theoretical framework for building upon Feynman's ideas.\n",
            "\n",
            "* **Peter Shor (born 1959):**  In 1994, Shor developed a quantum algorithm that could efficiently factor large numbers, a problem considered computationally intractable for classical computers. This algorithm had a profound impact, demonstrating the potential of quantum computers to break widely used encryption methods and spurring significant interest and investment in the field.\n",
            "\n",
            "* **Lov Grover (born 1961):**  In 1996, Grover developed a quantum algorithm for searching unsorted databases quadratically faster than the best classical algorithms.  While not as revolutionary as Shor's algorithm, it demonstrated another significant advantage of quantum computation.\n",
            "\n",
            "\n",
            "Many other researchers have made crucial contributions to the development of quantum computing hardware, algorithms, and theory.  The field is highly collaborative and continues to evolve rapidly.  These individuals are often highlighted because their work represents major breakthroughs that shaped the direction and possibilities of the field.\n",
            "We discussed the difference between classical and quantum computers using the analogy of a light switch.  A classical bit is like a light switch: it's either on (1) or off (0).  A qubit, however, can be both on and off simultaneously due to superposition.  Furthermore, qubits can be entangled, meaning their fates are intertwined, allowing quantum computers to explore many possibilities at once. This makes them potentially much faster than classical computers for specific complex problems, although they are currently experimental and have specialized applications.  We also highlighted key figures in quantum computing's history: Richard Feynman (for proposing quantum simulation), David Deutsch (for a theoretical universal quantum computer), Peter Shor (for his factoring algorithm), and Lov Grover (for his database search algorithm).  Finally, we acknowledged that quantum computing is a collaborative field with many other significant contributors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "-Nk7MEKmI8uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ],
      "metadata": {
        "id": "LUCHli03I-dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rest everything is same process"
      ],
      "metadata": {
        "id": "aBg47aHQI_Bt"
      }
    }
  ]
}