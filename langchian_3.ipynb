{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMvVqKv0vRq47SEo92MGLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanushpachabhatla/My_LangChain_Playground/blob/main/langchian_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts Templating for Gemini - LangChain"
      ],
      "metadata": {
        "id": "J0pDeLsTSHs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langsmith --quiet"
      ],
      "metadata": {
        "id": "SHUnWiKGU5Tg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "GQmU1iX0U6x9",
        "outputId": "c0267970-eadd-4807-caa3-4a512a9b95aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.75 (from langchain_google_genai)\n",
            "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.4.16)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.10-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, langchain-core, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.74\n",
            "    Uninstalling langchain-core-0.3.74:\n",
            "      Successfully uninstalled langchain-core-0.3.74\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-core-0.3.75 langchain_google_genai-2.1.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "cd609bf570e645e99aba361f80604294"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "# below should not be changed\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# you can change this as preferred\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"pr-husky-bran-20\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjD-aBbGU7_6",
        "outputId": "559d43cf-ab99-4ef4-e5ae-34aa1e2521f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Prompting"
      ],
      "metadata": {
        "id": "F6XBJ1FlVUCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
        "\n",
        "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
        "\n",
        "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
        "\n",
        "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
        "\n",
        "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
        "\n",
        "The below is an example of how a RAG prompt may look:"
      ],
      "metadata": {
        "id": "j8BYJtT1VZd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Answer the question based on the context below,                 }\n",
        "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
        "provided information answer with \"I don't know\"                 }\n",
        "\n",
        "Context: Aurelio AI is an AI development studio                 }\n",
        "focused on the fields of Natural Language Processing (NLP)      }\n",
        "and information retrieval using modern tooling                  }--->   Context AI has\n",
        "such as Large Language Models (LLMs),                           }\n",
        "vector databases, and LangChain.                                }\n",
        "\n",
        "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
        "\n",
        "Answer:                                                         }--->   AI Answer\n",
        "```"
      ],
      "metadata": {
        "id": "5Lts-uA0V94a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Answer the user's query based on the context below.\n",
        "If you cannot answer the question using the\n",
        "provided information answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_1SkbaAvVSaO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM:"
      ],
      "metadata": {
        "id": "BLJcMGHqWG66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* manual usage of ChatPromptTemplate"
      ],
      "metadata": {
        "id": "zMvZcM6JXUJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "h4uY49K5WFWy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we call the template it will expect us to provide two variables, the `context` and the `query`. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's `input_variables` attribute:"
      ],
      "metadata": {
        "id": "Xr5gU8MHWosU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG_h1TARWshn",
        "outputId": "2ec3ee89-5fb8-417c-d5db-b358a7a63129"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['context', 'query']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYSCsrevWzlN",
        "outputId": "e54de6b6-77e1-4e32-a1d9-b185946fefb8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Template usage of ChatPromptTemplate"
      ],
      "metadata": {
        "id": "XQG7T7-rXYXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ( SystemMessagePromptTemplate ,HumanMessagePromptTemplate)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "prompt_template.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_43Z_EdW_70",
        "outputId": "537015eb-405a-4562-8950-12e3e559bf2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is much similar to the first one"
      ],
      "metadata": {
        "id": "Fr7mim6eXn2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hitting LLM"
      ],
      "metadata": {
        "id": "dR8gS_bhYXzj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulCWVvveRuSF",
        "outputId": "45c000f2-41ac-47b4-b81c-ba1742731d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GEMINI KEY··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = getpass(\"Enter your GEMINI KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash',temperature= 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"],\n",
        "        \"context\": lambda x: x[\"context\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "1Dk4hWBfYa7-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
        "engineers. Their focus is on language AI with the team having strong\n",
        "expertise in building AI agents and a strong background in\n",
        "information retrieval.\n",
        "\n",
        "The company is behind several open source frameworks, most notably\n",
        "Semantic Router and Semantic Chunkers. They also have an AI\n",
        "Platform providing engineers with tooling to help them build with\n",
        "AI. Finally, the team also provides development services to other\n",
        "organizations to help them bring their AI tech to market.\n",
        "\n",
        "Aurelio AI became LangChain Experts in September 2024 after a long\n",
        "track record of delivering AI solutions built with the LangChain\n",
        "ecosystem.\"\"\"\n",
        "\n",
        "query = \"what does Aurelio AI do?\""
      ],
      "metadata": {
        "id": "1QmVs0IrYfAF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.invoke({\"query\": query, \"context\": context})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R2WiFX_Yfvf",
        "outputId": "d098cd26-05af-4074-a481-783a457c6c67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Aurelio AI is an AI company that develops tooling for AI engineers, focusing on language AI.  They specialize in building AI agents and have expertise in information retrieval.  They've created open-source frameworks like Semantic Router and Semantic Chunkers, and offer an AI Platform with tools for building AI applications.  Additionally, they provide development services to help other organizations bring their AI technology to market.  They became LangChain Experts in September 2024.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--718e85fb-6454-4d8d-98bb-97d4fe3bffd1-0', usage_metadata={'input_tokens': 180, 'output_tokens': 95, 'total_tokens': 275, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few shot Prompting"
      ],
      "metadata": {
        "id": "j7sd2fc_Yq2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "UkoLoT7FYo18"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
        "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
        "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
        "]"
      ],
      "metadata": {
        "id": "9TtaP_zHZoGD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "# here is the formatted prompt\n",
        "print(few_shot_prompt.format())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Teo7G_7SZp-x",
        "outputId": "996b49d0-5e33-463f-ba9e-f4145e298af8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Here is query #1\n",
            "AI: Here is the answer #1\n",
            "Human: Here is query #2\n",
            "AI: Here is the answer #2\n",
            "Human: Here is query #3\n",
            "AI: Here is the answer #3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* real example"
      ],
      "metadata": {
        "id": "VAWrq9f1ajon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_system_prompt = \"\"\"\n",
        "Answer the user's query based on the context below.\n",
        "If you cannot answer the question using the\n",
        "provided information answer with \"I don't know\".\n",
        "\n",
        "Always answer in markdown format. When doing so please\n",
        "provide headers, short summaries, follow with bullet\n",
        "points, then conclude.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template.messages[0].prompt.template = new_system_prompt\n",
        "\n",
        "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyWyJNZAbDtl",
        "outputId": "9dad90a9-d7dc-43f3-dd12-c27135902d56"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Aurelio AI: An Overview\n",
            "\n",
            "Aurelio AI is an artificial intelligence company specializing in language AI and building tools for AI engineers.\n",
            "\n",
            "*   **Focus:** Language AI, AI agents, information retrieval.\n",
            "*   **Open-Source Frameworks:** Semantic Router and Semantic Chunkers.\n",
            "*   **AI Platform:** Provides tooling for AI engineers.\n",
            "*   **Development Services:** Helps organizations bring AI technology to market.\n",
            "*   **Expertise:** LangChain Experts (since September 2024).\n",
            "\n",
            "\n",
            "# Conclusion\n",
            "\n",
            "Aurelio AI offers a range of services and tools centered around language AI, from open-source frameworks to a comprehensive AI platform and development services.  Their expertise in LangChain further solidifies their position in the AI engineering landscape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "mImQZ_sYbv4l",
        "outputId": "f2909471-fa5f-48bd-c798-81c38100e091"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Aurelio AI: An Overview\n\nAurelio AI is an artificial intelligence company specializing in language AI and building tools for AI engineers.\n\n*   **Focus:** Language AI, AI agents, information retrieval.\n*   **Open-Source Frameworks:** Semantic Router and Semantic Chunkers.\n*   **AI Platform:** Provides tooling for AI engineers.\n*   **Development Services:** Helps organizations bring AI technology to market.\n*   **Expertise:** LangChain Experts (since September 2024).\n\n\n# Conclusion\n\nAurelio AI offers a range of services and tools centered around language AI, from open-source frameworks to a comprehensive AI platform and development services.  Their expertise in LangChain further solidifies their position in the AI engineering landscape."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "its not bad but what if i want it in some strict markdown pattern then i may use exmamples"
      ],
      "metadata": {
        "id": "8bWK8x9Eb4Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Can you explain gravity?\",\n",
        "        \"output\": (\n",
        "            \"## Gravity\\n\\n\"\n",
        "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
        "            \"### Discovery\\n\\n\"\n",
        "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
        "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
        "            \"### In General Relativity\\n\\n\"\n",
        "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
        "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
        "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
        "            \"### Gravitons\\n\\n\"\n",
        "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
        "            \"* They have not yet been detected.\\n\\n\"\n",
        "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the capital of France?\",\n",
        "        \"output\": (\n",
        "            \"## France\\n\\n\"\n",
        "            \"The capital of France is Paris.\\n\\n\"\n",
        "            \"### Origins\\n\\n\"\n",
        "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
        "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
        "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
        "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
        "        )\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "0SnODpi0ZxuX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")"
      ],
      "metadata": {
        "id": "OmJWJussaKcY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "_TquPftnaYuI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = few_shot_prompt.format()\n",
        "display(Markdown(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "dOk9W8nLaRr4",
        "outputId": "644e864d-a8bc-4064-fdc9-109eec8a4ad9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Human: Can you explain gravity?\nAI: ## Gravity\n\nGravity is one of the fundamental forces in the universe.\n\n### Discovery\n\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n\n### In General Relativity\n\n* Gravity is described as the curvature of spacetime.\n* The more massive an object is, the more it curves spacetime.\n* This curvature is what causes objects to fall towards each other.\n\n### Gravitons\n\n* Gravitons are hypothetical particles that mediate the force of gravity.\n* They have not yet been detected.\n\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n\n\nHuman: What is the capital of France?\nAI: ## France\n\nThe capital of France is Paris.\n\n### Origins\n\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we can inlclude fewshotprompting in our prommpt template like below"
      ],
      "metadata": {
        "id": "kBZfqhcAcLSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", new_system_prompt),\n",
        "    few_shot_prompt,\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "7V5FUIWoa3bI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = prompt_template | llm\n",
        "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
        "display(Markdown(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "D5e6-1y4a6GA",
        "outputId": "ec9f04a0-c9d3-44ba-b61a-4d35376e5432"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Aurelio AI: Building the Future of Language AI\n\nAurelio AI is an artificial intelligence company specializing in developing tools for AI engineers, with a particular focus on language AI.\n\n### Key Activities:\n\n* **Developing AI tooling:**  They create frameworks and platforms to assist in the building of AI applications.  Notable open-source projects include Semantic Router and Semantic Chunkers.\n* **Providing an AI Platform:** This platform offers engineers a suite of tools to streamline their AI development processes.\n* **Offering Development Services:** Aurelio AI helps other organizations bring their AI technologies to market through custom development services.\n* **LangChain Expertise:** Recognized as LangChain Experts since September 2024, demonstrating a strong track record of building AI solutions within the LangChain ecosystem.\n* **Strong Background in Information Retrieval and AI Agents:** The team possesses significant expertise in these areas, informing their development efforts.\n\n\n**In short,** Aurelio AI empowers AI engineers with the tools and expertise needed to build cutting-edge language AI applications."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chain Of Though Prompting"
      ],
      "metadata": {
        "id": "40myRWFRcUAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's look at Chain of Thought `CoT`, a prompting technique that gets Large Language Models `LLMs` to \"think\" step-by-step. Instead of just giving an answer, CoT encourages the LLM to break a problem down into smaller, manageable steps. This process makes the model more likely to reach the correct solution and significantly reduces the chance of it making things up, or \"hallucinating.\"\n",
        "\n",
        "You don't need any special tools from LangChain to use CoT. The magic happens in the system prompt itself. We'll simply instruct the LLM to outline the problems, solve each one individually, and then present the final answer.\n",
        "\n",
        "Ready to see the difference? Let's first test our LLM without this technique to see how it performs."
      ],
      "metadata": {
        "id": "XuglGfIacjsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we can add explicit CoT prompting to our system prompt to see if we can get a better result."
      ],
      "metadata": {
        "id": "dUVv7RvRc3Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\n",
        "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
        ")"
      ],
      "metadata": {
        "id": "AHxLkmRpdChX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chain-of-thought prompt template\n",
        "cot_system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\n",
        "To answer the question, you must:\n",
        "\n",
        "- List systematically and in precise detail all\n",
        "  subproblems that need to be solved to answer the\n",
        "  question.\n",
        "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
        "- Finally, use everything you have worked through to\n",
        "  provide the final answer.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", cot_system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])\n",
        "\n",
        "cot_pipeline = cot_prompt_template | llm"
      ],
      "metadata": {
        "id": "stfVFqqgc2Rl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
        "display(Markdown(cot_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "FzWW9MXXcYuP",
        "outputId": "f49caa1b-316c-4972-866d-198da10b253f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To determine the total number of keystrokes needed to type the numbers from 1 to 500, we need to solve the following subproblems:\n\n**Subproblem 1: Count keystrokes for numbers with one digit (1-9)**\n\n* There are 9 one-digit numbers.\n* Each number requires 1 keystroke.\n* Total keystrokes for one-digit numbers: 9 * 1 = 9 keystrokes\n\n**Subproblem 2: Count keystrokes for numbers with two digits (10-99)**\n\n* There are 90 two-digit numbers (99 - 10 + 1 = 90).\n* Each number requires 2 keystrokes.\n* Total keystrokes for two-digit numbers: 90 * 2 = 180 keystrokes\n\n**Subproblem 3: Count keystrokes for numbers with three digits (100-500)**\n\n* There are 401 three-digit numbers (500 - 100 + 1 = 401).\n* Each number requires 3 keystrokes.\n* Total keystrokes for three-digit numbers: 401 * 3 = 1203 keystrokes\n\n**Subproblem 4: Calculate the total keystrokes**\n\n* Total keystrokes = keystrokes for one-digit numbers + keystrokes for two-digit numbers + keystrokes for three-digit numbers\n* Total keystrokes = 9 + 180 + 1203 = 1392 keystrokes\n\n\n**Final Answer:**  A total of 1392 keystrokes are needed to type the numbers from 1 to 500."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this result will be better than normal prompting, now lets see what will be the answer if we dont do `COT`"
      ],
      "metadata": {
        "id": "iAbtdkl6dNAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])\n",
        "\n",
        "pipeline = prompt_template | llm"
      ],
      "metadata": {
        "id": "T8vUlZJfdRP2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.invoke({\"query\": query}).content\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "_EYmUIwudT3S",
        "outputId": "aed307c1-3934-4879-8729-24182b47af7c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Let's break this down:\n\n* **Numbers 1-9:**  Each number requires one keystroke, totaling 9 keystrokes.\n* **Numbers 10-99:** Each number requires two keystrokes, and there are 90 numbers in this range (99 - 10 + 1 = 90). This totals 90 * 2 = 180 keystrokes.\n* **Numbers 100-999:** Each number requires three keystrokes.  We're only interested in 100-500, which is 401 numbers (500 - 100 + 1 = 401). This totals 401 * 3 = 1203 keystrokes.\n\n\nAdding those together: 9 + 180 + 1203 = **1392 keystrokes**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though we got the correct answer, for some complex questions model may hallucinate and give wrong answers but using COT sometimes we get better result as that how human also give better answer when we solve step by step ~ giving more time/breaking into smaller parts"
      ],
      "metadata": {
        "id": "4Er-WHE_dkrL"
      }
    }
  ]
}